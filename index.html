<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="BEAT: Berkeley Emotion and Affect Tracking Dataset">
  <meta name="keywords" content="BEAT, Emotion and Affect Tracking, Berkeley, Dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BEAT: Berkeley Emotion and Affect Tracking Dataset</title>


  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0QTH00CBRE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0QTH00CBRE');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://veatic.github.io/">
            VEATIC
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">BEAT: Berkeley Emotion and Affect Tracking Dataset</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://albuspeter.github.io/">Zhihang Ren</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.jeffersonortega.me/">Jefferson Ortega</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://yfwang.me/">Yifan Wang</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              Ana Hernandez<sup>1</sup>,
            </span>
            <span class="author-block">
              Zhimin Chen<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://yunhuiguo.github.io/">Yunhui Guo</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://web.eecs.umich.edu/~stellayu/">Stella X. Yu</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://whitneylab.berkeley.edu/people/dave.html">David Whitney</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Berkeley</span>
            <span class="author-block"><sup>2</sup>University of Texas at Dallas</span>
            <span class="author-block"><sup>3</sup>University of Michigan, Ann Arbor</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(<sup>*</sup>Equal Contribution)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/WACV2024/html/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <!-- <a href="https://arxiv.org/abs/2309.06745"
                   class="external-link button is-normal is-rounded is-dark"> -->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/OQI82kLghvI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/AlbusPeter/VEATIC"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Poster Link. -->
              <span class="link-block">
                <a href="./static/images/BEAT_VSS2024_Poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Poster</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="column has-text-centered">
        <img src="./static/images/Teaser.png", width="500 px">
      </div>
      <h2 class="subtitle has-text-centered">
        Importance of context in emotion recognition.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Video_4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Video_28.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Video_63.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Video_73.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Video_78.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Video_79.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The ability to perceive the emotions of others is incredibly important when navigating and understanding the social world around us. To understand this visual perceptual mechanism, previous studies have focused on face processing leading many previous datasets to collect face-centric data. Recent research has found that the visual system also utilizes background scene context to modulate and assign perceived emotion. Similarly, “in-the-wild” datasets have been created to include contextual information, such as CAER and EMOTIC. However, these datasets only track categorical emotions, and ignore dimensional ratings (i.e. valence and arousal) or collect ratings on static images. In this project, we propose BEAT: The Berkeley Emotion and Affect Tracking Dataset, the first video-based dataset that contains both categorical and continuous emotion annotations for a large number of videos (124 videos total). BEAT provides more insights into human emotion perception by providing both categorical and dimensional ratings for individual videos. Additionally, BEAT can help researchers better understand how emotion is processed temporally, as it is in the real world. Additionally, compared to other datasets, a large number of annotators (n = 245) were recruited to avoid idiosyncratic biases. BEAT can also be beneficial for artificial intelligence (AI) models. Using unbiased and multi-modality annotations, AI models trained on BEAT can be more robust and fair. Finally, we also release a new AI benchmark for emotion recognition multi-tasking. The BEAT dataset will help increase our understanding about how humans perceive the emotions of others in natural scenes.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset Preview</h2>
        <div class="hero-body">
          <img src="./static/images/preview.png", width="800 px", align="center">
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Annotation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Annotations</h2>

        <!-- Process. -->
        <h3 class="title is-4">Procedures</h3>
        <div class="content has-text-justified">
          <p>
            User interface used for video annotation. a) Participants were first shown the target character and were reminded of the task
            instructions before the start of each video. b) The overlayed valence and arousal grid that was present while observers annotated the
            videos. Observers were instructed to continuously rate the emotion of the target character in the video in real-time.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/annotation.png", width="800 px", align="center">
        </div>
        <!--/ Process. -->

      </div>
    </div>
    <!--/ Annotation. -->

    <div class="columns is-centered">

      <!-- Sample Ratings. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Sample Ratings</h2>
          <div class="column has-text-centered">
            <img src="./static/images/sample_ratings.png", width="450 px", align="center">
          </div>
          <p>
            Example valence and arousal ratings for a single video (video 47). Transparent gray lines indicate individual subject ratings
            and the green line is the average rating across participants.
          </p>
        </div>
      </div>
      <!--/ Sample Ratings. -->

      <!-- Annotation Distribution. -->
      <div class="column">
        <h2 class="title is-3">Data Distributions</h2>

        <div class="columns is-centered">
          <div class="column content">
            <div class="column has-text-centered">
              <img src="./static/images/VA_dist.png", width="500 px", align="center">
            </div>
            <p>
              Distribution of valence and arousal ratings across participants.
              Individual white dots represent the average valence and
              arousal of the continuous ratings for each video clip for Hollywood
              movies. Blue squares and green triangles represent the average
              valence and arousal for documentaries and home videos, respectively.
            </p>
          </div>
        </div>

        <div class="columns is-centered">
          <div class="column content">
            <div class="column has-text-centered">
              <img src="./static/images/Pie.png", width="500 px", align="center">
            </div>
            <p>
              Distribution of 11 categorical emotion states across participants.
            </p>
          </div>
        </div>
      </div>
      <!--/ Annotation Distribution. -->

    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Downloads. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Download (TBD)</h2>
        <!-- <div class="content has-text-justified">
          <p>
            <strong>VEATIC is available to download for research purposes.</strong>
          </p>
          <p>
            <strong>The copyright remains with the original owners of the video.</strong>
          </p>
        </div> -->

        <!-- VEATIC Data. -->
        <!-- <h3 class="title is-4">VEATIC Data</h3>
        <div class="content has-text-justified">
          <p>
            This benchmark contains 124 annotated videos.
          </p>
          <p>
            For each video, the first 70% of the frames are for training and the rest 30% of the frames are for testing. 
            You can directly utilize the dataset class provided in our baseline repository for training and testing your own model.
          </p>
        </div>
        <div class="content has-text-justified">
          <button class="btn" onclick="location.href='https://drive.google.com/file/d/1HZIw8RGsRwwENhJlhNJRL88YyfiE442N/view?usp=sharing'"><i class="fa fa-download"></i>VEATIC</button>
        </div>
        <div class="content has-text-justified">
          <p>
            VEATIC also contains videos with interacting characters and ratings for separate characters in the same video. These videos are those with video IDs 98-123.
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/novelrating.png", width="600 px", align="center">
        </div> -->
        <!--/ VEATIC Data. -->

        <!-- Psychophysics. -->
        <!-- <h3 class="title is-4">Psychophysics Data</h3>
        <div class="content has-text-justified">
          <p>
            We also provide individual ratings of each video with video IDs 0-82 for psychophysical studies. These ratings can be found below. 
            Each column of a file represents one individual's ratings, and same individuals can be identified via their ID markers.
          </p>
        </div>
        <div class="content has-text-centered">
          <button class="btn" onclick="location.href='https://drive.google.com/file/d/1UJ2A5ZJxOzKLW63T_S8DvZ1C8NrSrMgI/view?usp=sharing'"><i class="fa fa-download"></i>VEATIC_Psych</button>
        </div> -->
        <!--/ Psychophysics. -->

        <!-- Target. -->
        <!-- <h3 class="title is-4">Target Characters</h3>
        <div class="content has-text-justified">
          <p>
            You can find the target characters of each video below. We circled out the target character with a red circle in a frame of each video.
          </p>
        </div>
        <div class="content has-text-centered">
          <button class="btn" onclick="location.href='https://drive.google.com/file/d/1V7jHbeVTSBN8M0IG7-CeHKAWG6b6W4AB/view?usp=sharing'"><i class="fa fa-download"></i>Characters</button>
        </div> -->
        <!--/ Target. -->

      </div>
    </div>
    <!--/ Downloads. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX (TBD)</h2>
    <!-- <pre><code>
      @article{ren2023veatic,
        title    = {VEATIC: Video-based Emotion and Affect Tracking in Context Dataset},
        author   = {Ren, Zhihang and Ortega, Jefferson and Wang, Yifan and Chen, Zhimin and Whitney, David and Guo, Yunhui and Yu, Stella X},
        journal  = {arXiv preprint arXiv:2309.06745},
        year     = {2023}
      }
    </code></pre> -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/AlbusPeter" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website refers to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies website</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
